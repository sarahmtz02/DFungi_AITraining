# Clasificaci√≥n de Im√°genes de Infecciones por Hongos con TensorFlow

<p align="right">
    <img src="Fotos/tec_logo.png" alt="Logo Tec" width="260"/>
</p>

## üìù Descripci√≥n

Proyecto acad√©mico para la materia Desarrollo de aplicaciones avanzadas de ciencias computacionales. Este repositorio implementa un modelo de aprendizaje supervisado con TensorFlow para la clasificaci√≥n de im√°genes m√©dicas en cinco clases distintas de infecciones por hongos.
Se ha desarrollado con la t√©cnica de **Aprendizaje Supervisado**. Esta es una sub√°rea del Machine Learning en la que el modelo se entrena a identificar patrones aliment√°ndose de un conjunto de datos con etiquetas (en este caso, utilizando 3 categor√≠as). Se busca que el sistema realice una tarea de clasificaci√≥n donde pueda identificar a cu√°l de las 3 categor√≠as pertenece cada una de las im√°genes

## Introducci√≥n

En la √∫ltima d√©cada, la Inteligencia Artificial (IA) se ha desarrollado y establecido como herramienta fundamental en m√∫ltiples campos tanto cient√≠ficos como tecnol√≥gicos. Particularmente, se ha avanzado sustancialmente en tareas de procesamiento de datos complejos, como el an√°lisis de im√°genes. Las redes neuronales artificiales, en especial las redes neuronales convolucionales (CNN, por sus siglas en ingl√©s), han demostrado la capacidad para la extracci√≥n autom√°tica de caracter√≠sticas relevantes a partir de im√°genes, facilitando la automatizaci√≥n de procesos anteriormente reservados a revisi√≥n manual por parte de ingenieros u otros profesionales.
De entre las aplicaciones m√°s relevantes de las CNN, en este trabajo nos concentramos en la clasificaci√≥n de im√°genes como tarea cr√≠tica en uno de los muchos posibles contextos m√©dicos y cl√≠nicos. Este tipo de redes es especialmente adecuado para identificar patrones visuales complejos, lo que las convierte en una herramienta poderosa en el diagn√≥stico por imagen. La estructura jer√°rquica que toman las CNN permite procesar la informaci√≥n visual desde niveles b√°sicos como bordes y texturas, hasta representaciones de alto nivel entre ellas formas y estructuras celulares, permitiendo la identificaci√≥n automatizada de patolog√≠as a partir de muestras visuales.
En el √°rea de la microbiolog√≠a m√©dica, el uso de estas redes neuronales para la clasificaci√≥n de im√°genes ha probado ser eficaz en la detecci√≥n de infecciones bacterianas y f√∫ngicas. Estos modelos son entrenados con bases de datos (o datasets) de im√°genes cl√≠nicas o de laboratorio y han logrado resultados confiables en la identificaci√≥n de agentes pat√≥genos en im√°genes.
La incorporaci√≥n de estas tecnolog√≠as en el diagn√≥stico cl√≠nico ha tenido un gran impacto en t√©rminos de eficiencia, accesibilidad y reducci√≥n de errores humanos. En particular, en entornos con recursos limitados o escasez de especialistas, los sistemas de clasificaci√≥n automatizada pueden ser un apoyo valioso al personal m√©dico, logrando diagn√≥sticos m√°s r√°pidos y acertados. Asimismo, estos modelos pueden ser integrados en plataformas digitales, facilitando la toma de decisiones cl√≠nicas en tiempo real. En el presente trabajo se enmarca en este contexto y tiene como objetivo desarrollar un modelo de red neuronal convolucional para la clasificaci√≥n de im√°genes de muestras cl√≠nicas, orientado a la detecci√≥n de infecciones de origen f√∫ngico. Este proyecto busca contribuir y entender el desarrollo de herramientas de apoyo para diagn√≥stico automatizado, promoviendo la adopci√≥n de tecnolog√≠as basadas en IA en entornos cl√≠nicos.

## State of the Art

Referencias del Estado del Arte (Papers):
J. Baral, A. K. Baral, S. K. Baral, D. K. Baral, and S. K. Baral, "Deep ensemble learning for bacterial colony classification," Frontiers in Microbiology, vol. 14, 2023, Art. no. 10173177. [Online]. Available: https://doi.org/10.3389/fmicb.2023.10173177

Y. Gao, X. Zhang, Y. Wang, and Z. Li, "Vision Transformer for fungal infection detection in time-lapse microscopy," Neural Networks, vol. 168, pp. 1‚Äì12, 2023. [Online]. Available: https://doi.org/10.1016/j.neunet.2023.07.219

Ha habido m√∫ltiples trabajos, proyectos e investigaciones que han apuntado su atenci√≥n a este tema. En los precedentes de estos experimentos cuando iban empezando, los modelos se apoyaban en m√©todos de visi√≥n por computador basados en descriptores manuales como:

- SIFT (Scale-Invariant Feature Transform)
  Detecta y describe puntos clave en una imagen que son invariantes a escala, rotaci√≥n e iluminaci√≥n. Es √∫til para reconocer objetos o patrones en im√°genes incluso si han sido rotados o escaladas.

- HOG (Histogram of Oriented Gradients)
  Divide la imagen en celdas peque√±as y computa histogramas de gradientes (direcciones de bordes). Fue muy usado en detecci√≥n de peatones y estructuras anat√≥micas simples, ya que captura bien las formas y contornos locales.

- LBP (Local Binary Patterns)
  Analiza la textura local de una imagen comparando cada p√≠xel con sus vecinos. Si un p√≠xel vecino es mayor que el central, se asigna un 1, en caso contrario un 0. Estos patrones binarios se usan para describir texturas y estructuras microsc√≥picas, como superficies celulares.

Sin embargo, la irrupci√≥n de las redes neuronales convolucionales (CNN) propici√≥ una transici√≥n hacia sistemas capaces de aprender representaciones jer√°rquicas directamente de los datos. Desde entonces, arquitecturas profundas como, por ejemplo, VGG, ResNet y DenseNet se han convertido en el est√°ndar para tareas de diagn√≥stico por imagen como las que estamos trabajando en este proyecto. En el √°mbito bacteriano, los estudios recientes evidencian mejoras sustanciales cuando se usan arquitecturas profundas.

## üìÇ Sobre el Dataset

Recuperado del link: https://archive.ics.uci.edu/dataset/773/defungi
Autores: Mar√≠a Alejandra Vanegas √Ålvarez, Leticia Sop√≥, C. Sopo, F. Hajati, S. Gheisari

La estructura del dataset es la siguiente:

```
DFungi Dataset
‚îú‚îÄ‚îÄ test
‚îÇ ‚îú‚îÄ‚îÄ H1 (3563 im√°genes)
‚îÇ ‚îú‚îÄ‚îÄ H2 (1887 im√°genes)
‚îÇ ‚îî‚îÄ‚îÄ H3 (667 im√°genes)
‚îî‚îÄ‚îÄ train
‚îÇ ‚îú‚îÄ‚îÄ H1 (891 im√°genes)
‚îÇ ‚îú‚îÄ‚îÄ H2 (474 im√°genes)
‚îÇ ‚îî‚îÄ‚îÄ H3 (162 im√°genes)
‚îî‚îÄ‚îÄ augmented (312 im√°genes)
```

Each of the sections represents the following fungi types:
H1: Candida albicans
Es un hongo que causa principalmente candidiasis, una infecci√≥n que puede afectar diversas partes del cuerpo.

H2: Aspergillus niger
Es un hongo que, en altas concentraciones, puede producir aspergilosis invasiva que puede causar sangrado pulmonar grave y, a veces, mortal.

H3: Trichophyton rubrum
Es un hongo que causa dermatofitosis o ti√±as, principalmente en la piel y las u√±as, pero tambi√©n puede afectar el pelo.

<p>
    <img src="Fotos/H1.png" alt="H1" width="260" height="170"/>
    <img src="Fotos/H2.jpg" alt="H2" width="260" height="170"/>
    <img src="Fotos/H3.jpg" alt="H3" width="260" height="170"/>
</p>

## üîÅ Data Augmentation

Para mejorar la generalizaci√≥n del modelo y evitar el sobreajuste debido al bias, la t√©cnica de Data Augmentation. El proceso incluy√≥ rotaciones de hasta 10 grados, desplazamientos horizontales de la imagen de hasta un 20%, zoom de hasta un 30% y volteo horizontal. Todo para permitir al modelo generar variantes de las im√°genes alimentadas al inicio, enriqueciendo el dataset de entrenamiento sin necesidad de recolectar m√°s datos.

```
# Usamos la funci√≥n ImageDataGenerator de TensorFlow
train_datagen = ImageDataGenerator(
    rescale = 1./255, # Reescalamos las im√°genes
    rotation_range = 10, # Las rotamos levemente (10 grados)
    width_shift_range = 0.2, # Permitimos que se ensanche la imagen
    zoom_range = 0.3, # Hacemos zoom
    horizontal_flip = True, #Volteamos la imagen
    validation_split=0.10
    )
```

Tras esto, guardamos las im√°genes modificadas en la carpeta "augmented" como archivos .png. Esta funci√≥n tiene un l√≠mite de crear 300 im√°genes, debido a que sin ponerle l√≠mites, creaba m√°s de 10,000 archivos. Esto alentaba el entrenamiento del modelo y, posteriormente, el guardado del mismo debido a la alta densidad de archivos.

```
temp_sampler_generator = train_datagen.flow_from_directory(
    "../DFungi_dataset/train",
    target_size=(224, 224),
    batch_size=saving_batch_size,
    class_mode='categorical',
    save_to_dir=sample_augmented_dir,
    save_prefix='aug',
    save_format='png',
    subset='training', # Use 'training' if validation_split is in train_datagen
    shuffle=False, # Keep shuffle=False for consistent sample generation
)
```

## üßπ Preprocesamiento de Datos

Para preparar las im√°genes antes de entrenar el modelo, se utiliz√≥ la clase ImageDataGenerator de TensorFlow con un reescalado de entre 0 y 1, dividiendo cada valor por 255. Esto con el prop√≥sito de permitir una mejor convergencia del modelo durante el entrenamiento. Tambi√©n, se hizo un redimensionamiento de las im√°genes como el hecho en clase a un tama√±o uniforme de 244x244 p√≠xeles, seg√∫n lo estipulado en la arquitectura VGG16 que se revisar√° m√°s adelante, para asegurar la compatibilidad con la arquitectura de la red neuronal. Anteriormente, el modelo contaba con un recorte de las im√°genes para hacer enfoque s√≥lo de la zona de inter√©s para el entrenamiento, algo que definitivamente apoy√≥ bastante. Finalmente, se colocan los datos en batches y asigna etiquetas de clase en modo categ√≥rico debido a que esto es una clasificaci√≥n multiclase (tenemos 3 clases) en vez de las usadas en clase que eran binarias (s√≥lo 2 clasificaciones).

```
validation_generator = train_datagen.flow_from_directory(
    train_dir, # le damos el path de entrenamiento
    target_size = (224, 224), # Tama√±o de las im√°genes seg√∫n VGG16
    # batch_size = 1 porque la RAM es un relajo XD
    subset='validation',
    batch_size = 1, # la cantidad de im√°genes por conversi√≥n
    class_mode ='categorical', # modo categ√≥rico porque tenemos
    # 3 clases datos

test_generator = test_datagen.flow_from_directory(
    test_dir, # le damos el path de test
    target_size = (224, 224), # Tama√±o de las im√°genes seg√∫n VGG16
    # batch_size = 1 porque la RAM es un relajo XD
    batch_size = 1, # la cantidad de im√°genes por conversi√≥n
    class_mode ='categorical', # modo categ√≥rico porque tenemos
    # 3 clases datos
    )

train_generator = train_datagen.flow_from_directory(
    train_dir,# le damos el path de entrenamiento
    target_size = (224, 224), # Tama√±o de las im√°genes seg√∫n VGG16
    batch_size = 8, # la cantidad de im√°genes por conversi√≥n
    class_mode ='categorical', # modo categ√≥rico porque tenemos
    # 3 clases datos
    )
```

## VGG16 - Arquitectura del Modelo

Entre las arquitecturas m√°s utilizadas, VGG16 destaca por su simplicidad estructural y su capacidad para lograr resultados competitivos en este tipo de tareas, debido a que consiste de empalmar capas sobre capas para lograr la arquitectura profunda que permite el aprendizaje supervisado eficiente. Fue una arquitectura introducida en 2014 por Simoyan y Zisserman. Se compone de 16 capas con pesos entrenables y utiliza filtros peque√±os (3√ó3) con padding para conservar la dimensi√≥n espacial. Su dise√±o secuencial ha comprobado ser particularmente adecuado para problemas de detecci√≥n de infecciones en im√°genes microsc√≥picas, debido a su capacidad para captar caracter√≠sticas finas en las im√°genes como aquellas que usaremos para entrenar este modelo.
Las im√°genes con las que trabajan los pesos de ImageNet es de 224\*224, por lo que tenemos un input de (224, 224, 3) como tensor (el 3 por los filtros previamente mencionados). VGG16 se desglosa de la siguiente manera:

- Capa de entrada:
  Dimensiones de entrada: (224, 224, 3)

- Capas convolucionales (64 filtros, filtros 3x3, mismo relleno):
  Dos capas convolucionales consecutivas con 64 filtros cada una y un tama√±o de filtro de 3x3.
  Se aplica el mismo relleno para mantener las dimensiones espaciales.

- Capa de agrupaci√≥n m√°xima (2x2, paso 2):
  Capa de agrupaci√≥n m√°xima con un tama√±o de grupo de 2x2 y un paso de 2.

- Capas convolucionales (128 filtros, filtros 3x3, mismo relleno):
  Dos capas convolucionales consecutivas con 128 filtros cada una y un tama√±o de filtro de 3x3.

- Capa de agrupaci√≥n m√°xima (2x2, paso 2):
  Capa de agrupaci√≥n m√°xima con un tama√±o de grupo de 2x2 y un paso de 2.

- Capas convolucionales (256 filtros, 3x3 filtros, mismo relleno):
  Dos capas convolucionales consecutivas con 256 filtros cada una y un tama√±o de filtro de 3x3.

- Capas convolucionales (512 filtros, 3x3 filtros, mismo relleno):
  Dos conjuntos de tres capas convolucionales consecutivas con 512 filtros cada una y un tama√±o de filtro de 3x3.

- Capa de agrupaci√≥n m√°xima (2x2, paso 2):
  Capa de agrupaci√≥n m√°xima con un tama√±o de grupo de 2x2 y un paso de 2.

- Pila de capas convolucionales y agrupaci√≥n m√°xima:
  Dos capas convolucionales adicionales a la pila anterior.
  Tama√±o del filtro: 3x3.

- Aplanamiento:
  Aplanar el mapa de caracter√≠sticas de salida (7x7x512) en un vector de tama√±o 25088.

- Capas completamente conectadas:
  Tres capas completamente conectadas con activaci√≥n ReLU\*.

- Primera capa con tama√±o de entrada 25088 y tama√±o de salida 4096.

- Segunda capa con tama√±o de entrada 4096 y tama√±o de salida 4096.

- Tercera capa con tama√±o de entrada 4096 y tama√±o de salida 1000, correspondiente a las 1000 clases del desaf√≠o ILSVRC.
  Se aplica la activaci√≥n Softmax al output de la tercera capa completamente conectada para la clasificaci√≥n.

\*ReLU: es una funci√≥n de activaci√≥n que proporciona no linealidad al modelo para un mejor rendimiento de c√°lculo. Da como resultado el m√°ximo entre su entrada y cero. Para entradas positivas, la salida de la funci√≥n es igual a la entrada. Para salidas estrictamente negativas, la salida de la funci√≥n es igual a cero.

```
from tensorflow.keras.applications import VGG16
from tensorflow.keras import layers, models

input_shape = (224, 224, 3)  # Tama√±o de entrada de las im√°genes seg√∫n VGG16

# Load VGG16 with pre-trained ImageNet weights, excluding the top (classifier) layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

# Freeze base model layers so they are not trained
base_model.trainable = False

# Unfreeze some of the top layers
# For VGG16, 'block5_conv1' onwards are typically good candidates for unfreezing
# You can print base_model.summary() to see all layer names
for layer in base_model.layers:
    if layer.name.startswith('block5'): # Unfreeze layers in block5
        layer.trainable = True

model = models.Sequential()
model.add(base_model)
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))  # Optional: add more dense layers
model.add(layers.Dense(3, activation='softmax'))

model.summary()

# Compilar el modelo
model.compile(loss='categorical_crossentropy', # Categorical Crossentropy for multi-class classification with one-hot labels
						optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
						metrics=['accuracy']) # Accuracy to evaluate performance
```

De esta manera, la red queda como se muestra en la imagen:

<p>
    <img src="Fotos/red.png" alt="tabla_red"/>
</p>

Hubo una arquitectura previa en la cual se montaron las capas manualmente con so de MaxPooling2D despu√©s de cada capa convolutiva donde los filtros van escalando en valores de 2 elevado a las primeras potencias. Es un entrenamiento mucho m√°s lento y lo m√°ximo que se logr√≥ de accuracy fue de 61%. Est√° basado justamente en la arquitectura VGG16, pero justo con el planteamiento de que fuese m√°s r√°pido de entrenar y procesar en su momento.

```
from tensorflow.keras import optimizers
from tensorflow.keras import models
from tensorflow.keras import layers

input_shape = (224,224,3)
model = models.Sequential()

model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape))
model.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(32, (3, 3), activation='relu'))
model.add(layers.Conv2D(32, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.add(layers.Flatten())

num_classes = 3
model.add(layers.Dense(num_classes, activation='softmax'))

model.summary()

model.compile(loss='categorical_crossentropy', # Categorical Crossentropy for multi-class classification with one-hot labels
            optimizer='adam',
            metrics=['accuracy']) # Accuracy to evaluate performance
```

Posteriormente, en la b√∫squeda de mejorar la precisi√≥n del modelo, se hizo la propuesta de a√±adir los pesos a los filtros. Una vez con esta decisi√≥n, se implement√≥ la arquitectura VGG16 en su entereza con los pesos de ImageNet que son los pesos de default. Finalmente, tras la nueva implementaci√≥n (que es la primera que podemos ver en esta secci√≥n) el accuracy del modelo en TRAINING subi√≥ a 75%. Eso quiere decir que, ante la soluci√≥n de una arquitectura profunda, el modelo mejor√≥ UN 14% en accuracy de TRAINING.

# M√©tricas y Resultados

Para evaluar el desempe√±o de una red neuronal convolucional (CNN) en tareas de clasificaci√≥n o en cualquier caso, requiere el uso de m√©tricas particulares. ¬øC√≥mo se puede mejorar si no se sabe con exactitud el estado actual del proyecto? Esto es especialmente relevante en contextos donde los datos pueden estar desbalanceados, o cuando los errores tienen implicaciones importantes. Las m√©tricas que se utilizaron para analizar el rendimiento de este modelos son los siguientes:

- Precisi√≥n:
  Indica qu√© proporci√≥n de las predicciones positivas realizadas por el modelo son realmente correctas. Es √∫til cuando se desea minimizar los falsos positivos, por ejemplo, evitando diagnosticar err√≥neamente una infecci√≥n cuando no es correcta. Si el modelo identifica una imagen como perteneciente a una clase, ¬øcu√°ntas veces acert√≥?

- Sensibilidad o "recall":
  Mide la capacidad del modelo para identificar correctamente todos los casos positivos reales. Esta m√©trica es clave en contextos m√©dicos, donde pasar por alto un caso positivo (falso negativo) puede ser cl√≠nicamente riesgoso. Se resume a la pregunta, de todas las im√°genes que realmente muestran infecci√≥n, ¬øcu√°ntas detect√≥ el modelo?

- F1:
  Combina los resultados tanto la precisi√≥n como la sensibilidad en una sola m√©trica. Es especialmente √∫til cuando existe un desbalance entre clases o cuando se necesita un equilibrio entre los falsos positivos y los falsos negativos.

- Soporte:
  Representa el n√∫mero total de muestras reales que pertenecen a cada clase. Aunque no es una m√©trica de rendimiento por s√≠ misma, proporciona contexto sobre la confiabilidad de las m√©tricas anteriores, ya que los valores calculados sobre clases con muy pocos ejemplos pueden ser inestables o poco representativos.

Estas m√©tricas suelen presentarse de forma desglosada por clase en tablas de reporte, lo que permite un an√°lisis detallado del comportamiento del modelo. En el caso de este modelo, se utilizaron las m√©tricas mencionadas anteriormente al igual que una matriz de confusi√≥n. Como lo menciona su nombre, representa gr√°ficamente qu√© tanto se "confundi√≥" y acert√≥ el modelo, comparando el resultado esperado con el que predijo el modelo.

<p>
    <img src="Fotos/metrics.png" alt="metricas_modelo"/>
</p>

Como se puede ver en la tabla de arriba, el modelo tiene un desempe√±o general bajo. ¬øPero qu√© no se hab√≠a mencionado antes que se ten√≠a un accuracy de 75%? ¬øQu√© pas√≥? El valor de 75% sale de los archivos seleccionados para entrenar el modelo. Eso quiere decir que son archivos que el mismo modelo reconoce y puede identificar con mayor facilidad. Sin embargo, las pruebas para las m√©tricas usan los archivos del dataset en la carpeta de TEST. Im√°genes que el modelo nunca ha visto y batalla m√°s en reconocer. El accuracy final de la prueba con TEST fue de 0.38, lo que implica que menos del 40 % de las predicciones fueron correctas en relaci√≥n al total de muestras. Esta cifra est√° por debajo del umbral de utilidad pr√°ctica para la mayor√≠a de las aplicaciones.

En desglose:
La clase H1 obtuvo el mejor rendimiento relativo, con una precisi√≥n de 0.45 y una sensibilidad (recall) de 0.66. Esto quiere decir que el modelo identifica correctamente la mayor√≠a de los casos reales de H1, aunque tambi√©n comete un n√∫mero considerable de falsos positivos.

La clase H2 mostr√≥ un desempe√±o bajo, especialmente en recall (0.14), lo cual sugiere que el modelo est√° fallando en reconocer la mayor√≠a de los casos de esta clase. Su precisi√≥n es ligeramente mejor (0.39), lo que indica que cuando predice H2, no lo hace con demasiada frecuencia, pero s√≠ con algo m√°s de acierto.

La clase H3 fue la m√°s d√©bilmente clasificada, con precisi√≥n, recall y F1-score todos por debajo de 0.25. Esto es un fuerte indicio de que el modelo apenas est√° reconociendo los casos de esta clase y, cuando lo hace, no es confiable.

Todo esto se puede observar a trav√©s de la matriz de confusi√≥n m√°s abajo:

<p>
    <img src="Fotos/matrix.png" alt="matrix" height="500"/>
</p>

Finalmente, los promedios macro y ponderado (macro avg y weighted avg) muestran consistencia con los datos por clase, revelando un rendimiento bajo y un desequilibrio entre clases. El F1-score ponderado de 0.34 indica que incluso teniendo en cuenta el n√∫mero de muestras por clase, el modelo no logra un rendimiento satisfactorio.
Los resultados obtenidos muestran que el modelo a√∫n presenta deficiencias en su capacidad de clasificaci√≥n, especialmente en lo que respecta a las clases H2 y H3. Aunque la clase H1 presenta m√©tricas m√°s aceptables, el bajo desempe√±o en las otras dos clases afecta negativamente el resultado global. ¬øSi tiene falsos positivos de H1 para la gran mayor√≠a de las im√°genes, qu√© tanto podr√≠amos confiar en √©l? Adem√°s, la baja accuracy (38 %) y los bajos valores de las dem√°s m√©tricas sugieren que el modelo no generaliza bien y tiene dificultades para distinguir entre patrones de las distintas clases.

Tambi√©n, se hicieron las gr√°ficas respecto a las comparaciones del TRAINING y VALIDATION en tanto el accuracy como el loss del modelo.

<p>
    <img src="Fotos/acc.png" alt="matrix" height="500"/>
</p>
<p>
    <img src="Fotos/loss.png" alt="matrix" height="500"/>
</p>

El modelo muestra se√±ales claras de OVERFITTING. Si seguimos ambas l√≠neas en las gr√°ficas, podemos observar que las l√≠neas no tienen muchas curvas. Tambi√©n, la precisi√≥n y p√©rdida en validaci√≥n fluct√∫a mucho y permanece baja durante todas las √©pocas. Y la diferencia que existe entre el TRAINING y el VALIDATION demuestra que el training va much√≠simo mejor, adem√°s de que las propias m√©tricas nos presentan el problema de que el modelo aprende patrones espec√≠ficos del conjunto de entrenamiento y no se generaliza bien a datos nuevos como los que se encuentran el TEST. En resumen, que el modelo est√° memorizando en vez de aprender a identificar las im√°genes.
